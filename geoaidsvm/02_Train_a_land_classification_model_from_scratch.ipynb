{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a land classification model from scratch for ECC\n",
    "\n",
    "\n",
    "This tutorial will use a provisioned an NC series [Geo AI Data Science Virtual Machine](http://aka.ms/dsvm/GeoAI) and are using this Jupyter notebook while connected via remote desktop on that VM. If not, please see our guide to [provisioning and connecting to a Geo AI DSVM](https://github.com/Azure/pixel_level_land_classification/blob/master/geoaidsvm/setup.md).\n",
    "\n",
    "\n",
    "The following command will use the [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy) utility to download sample data, a pre-trained model, and code to your VM. The file transfer may take a couple of minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!AzCopy /Source:https://aiforearthcollateral.blob.core.windows.net/imagesegmentationtutorial /SourceSAS:\"?st=2018-01-16T10%3A40%3A00Z&se=2028-01-17T10%3A40%3A00Z&sp=rl&sv=2017-04-17&sr=c&sig=KeEzmTaFvVo2ptu2GZQqv5mJ8saaPpeNRNPoasRS0RE%3D\" /Dest:D:\\pixellevellandclassification /S\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python packages (should already be installed on Geo AI DSVM)\n",
    "\n",,
    "- `tifffile`: load and save TIFF images (available as a conda package on the conda-forge channel)\n",
    "- `cntk`: for training neural networks (Python wheels for Windows listed [here](https://docs.microsoft.com/cognitive-toolkit/setup-windows-python))\n",
    "- `gdal`: read specialized headers in our TIFF files that contain information on the region shown, geospatial coordinate system used, etc. (we recommend [Christoph Gohlke's Python wheel](https://www.lfd.uci.edu/~gohlke/pythonlibs/) for a fast install)\n",
    "- `pyproj`: to read PROJ.4-formatted geospatial projection information (available as a conda package in the default channel)\n",
    "- `basemap`: to help convert between lat-lon coordinates and row/column positions in our data files (available as a conda package on the conda-forge channel)\n",
    "\n",
    "## Perform training\n",
    "\n",
    "Before starting training, ensure that there are not any running processes making use of GPUs. (This may be the case if you have other programs or Jupyter notebooks running.) The code cell below checks your GPU status and running processes using `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "proc = subprocess.Popen('nvidia-smi', stdout=subprocess.PIPE)\n",
    "print(proc.stdout.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the command below  replacing `%num_gpus%` with the number of GPUs on our VM, which is one:\n",
    "\n",
    "Geo AI DSVM SKU name | Number of GPUs\n",
    ":----:|:----:\n",
    "NC6 | 1\n",
    "NC12 | 2\n",
    "NC24 | 4\n",
    "\n",
    "After editing the command appropriately, we are going to open a Windows command prompt (e.g. by clicking on the Start menu, typing \"Command Prompt\", and pressing Enter), paste in the command, and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpiexec -n %num_gpus% C:\\Anaconda\\python ^\n",
    "    D:\\pixellevellandclassification\\scripts\\train_distributed.py ^\n",
    "    --input_dir D:\\pixellevellandclassification\\training_data ^\n",
    "    --model_dir D:\\pixellevellandclassification\\models ^\n",
    "    --num_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this time, you can finish reading this notebook and monitor progress as follows:\n",
    "- In the command prompt where you launched the training, you should soon see output messages indicating the number of GPUs (\"nodes\") participating.\n",
    "- Using Task Manager, observe that a Python process has been spawned for each GPU and is using a substantial amount of memory.\n",
    "    - This tutorial uses eight pairs of training files. They occupy more space in memory than they do on disk due to decompression on loading.\n",
    "    - Because it takes so long to load files of this size, we've chosen to load them once at the beginning of training and hold them in memory for fast access. This is especially beneficial when training for more than one epoch.\n",
    "- Re-run the `nvidia-smi` cell above: you should see utilization of all GPUs (eventually resulting in high temperature and GPU memory usage) and one running process per GPU.\n",
    "\n",
    "When training is complete, the output messages at the command prompt should indicate the duration of the training epoch and the error rate on the training set during the epoch, e.g.\n",
    "```\n",
    "Finished Epoch[1 of 1]: [Training] loss = 0.127706 * 16000, metric = 3.59% * 16000 1421.583s ( 11.3 samples/s);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data access\n",
    "\n",
    "Near the beginning of the training script is a custom minibatch source specifying how the training data should be read and used. Our training data comprise pairs of TIF images. The first image in each pair is a four-channel (red, green, blue, near-infrared) aerial image of a region of the Chesapeake Bay watershed. The second image is a single-channel \"image\" corresponding to the same region, in which each pixel's value corresponds to a land cover label:\n",
    "- 0: Unknown land type\n",
    "- 1: Water\n",
    "- 2: Trees and shrubs\n",
    "- 3: Herbaceous vegetation\n",
    "- 4+: Barren and impervious (roads, buildings, etc.); we lump these labels together\n",
    "\n",
    "These two images in each pair correspond to the features and labels of the data, respectively. The minibatch source specifies that the available image pairs should be partitioned evenly between the workers, and each worker should load its set of image pairs into memory at the beginning of training. This ensures that the slow process of reading the input images is performed only once per training job. To produce each minibatch, subregions of a given image pair are sampled randomly. Training proceeds by cycling through the image pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model architecture\n",
    "The [model definition script](https://aiforearthcollateral.blob.core.windows.net/imagesegmentationtutorial/scripts/model_mini_pub.py) specifies the model architecture: a form of [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The input for this model will be a 256 pixel x 256 pixel four-channel aerial image (corresponding to a 256 meter x 256 meter region), and the output will be predicted land cover labels for the 128 m x 128 m region at the center of the input region. (Predictions are not provided at the boundaries due to edge effects.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that we have produced a trained model, we will test its performance in the following notebook on [applying your model to new aerial images](./03_Apply_trained_model_to_new_data.ipynb). 
    "\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
